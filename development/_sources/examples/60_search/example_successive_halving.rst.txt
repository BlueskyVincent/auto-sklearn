
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "examples/60_search/example_successive_halving.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_examples_60_search_example_successive_halving.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_examples_60_search_example_successive_halving.py:


==================
Successive Halving
==================

This advanced  example illustrates how to interact with
the SMAC callback and get relevant information from the run, like
the number of iterations. Particularly, it exemplifies how to select
the intensification strategy to use in smac, in this case:
`SuccessiveHalving <http://proceedings.mlr.press/v80/falkner18a/falkner18a-supp.pdf>`_.

This results in an adaptation of the `BOHB algorithm <http://proceedings.mlr.press/v80/falkner18a/falkner18a.pdf>`_.
It uses Successive Halving instead of `Hyperband <https://jmlr.org/papers/volume18/16-558/16-558.pdf>`_, and could be abbreviated as BOSH.
To get the BOHB algorithm, simply import Hyperband and use it as the intensification strategy.

.. GENERATED FROM PYTHON SOURCE LINES 17-26

.. code-block:: default

    from pprint import pprint

    import sklearn.model_selection
    import sklearn.datasets
    import sklearn.metrics

    import autosklearn.classification









.. GENERATED FROM PYTHON SOURCE LINES 27-29

Define a callback that instantiates SuccessiveHalving
=====================================================

.. GENERATED FROM PYTHON SOURCE LINES 29-86

.. code-block:: default



    def get_smac_object_callback(budget_type):
        def get_smac_object(
            scenario_dict,
            seed,
            ta,
            ta_kwargs,
            metalearning_configurations,
            n_jobs,
            dask_client,
            multi_objective_algorithm,  # This argument will be ignored as SH does not yet support multi-objective optimization
            multi_objective_kwargs,
        ):
            from smac.facade.smac_ac_facade import SMAC4AC
            from smac.intensification.successive_halving import SuccessiveHalving
            from smac.runhistory.runhistory2epm import RunHistory2EPM4LogCost
            from smac.scenario.scenario import Scenario

            if n_jobs > 1 or (dask_client and len(dask_client.nthreads()) > 1):
                raise ValueError(
                    "Please make sure to guard the code invoking Auto-sklearn by "
                    "`if __name__ == '__main__'` and remove this exception."
                )

            scenario = Scenario(scenario_dict)
            if len(metalearning_configurations) > 0:
                default_config = scenario.cs.get_default_configuration()
                initial_configurations = [default_config] + metalearning_configurations
            else:
                initial_configurations = None
            rh2EPM = RunHistory2EPM4LogCost

            ta_kwargs["budget_type"] = budget_type

            return SMAC4AC(
                scenario=scenario,
                rng=seed,
                runhistory2epm=rh2EPM,
                tae_runner=ta,
                tae_runner_kwargs=ta_kwargs,
                initial_configurations=initial_configurations,
                run_id=seed,
                intensifier=SuccessiveHalving,
                intensifier_kwargs={
                    "initial_budget": 10.0,
                    "max_budget": 100,
                    "eta": 2,
                    "min_chall": 1,
                },
                n_jobs=n_jobs,
                dask_client=dask_client,
            )

        return get_smac_object









.. GENERATED FROM PYTHON SOURCE LINES 87-89

Data Loading
============

.. GENERATED FROM PYTHON SOURCE LINES 89-95

.. code-block:: default


    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=1, shuffle=True
    )








.. GENERATED FROM PYTHON SOURCE LINES 96-98

Build and fit a classifier
==========================

.. GENERATED FROM PYTHON SOURCE LINES 98-130

.. code-block:: default


    automl = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=40,
        per_run_time_limit=10,
        tmp_folder="/tmp/autosklearn_sh_example_tmp",
        disable_evaluator_output=False,
        # 'holdout' with 'train_size'=0.67 is the default argument setting
        # for AutoSklearnClassifier. It is explicitly specified in this example
        # for demonstrational purpose.
        resampling_strategy="holdout",
        resampling_strategy_arguments={"train_size": 0.67},
        include={
            "classifier": [
                "extra_trees",
                "gradient_boosting",
                "random_forest",
                "sgd",
                "passive_aggressive",
            ],
            "feature_preprocessor": ["no_preprocessing"],
        },
        get_smac_object_callback=get_smac_object_callback("iterations"),
    )
    automl.fit(X_train, y_train, dataset_name="breast_cancer")

    pprint(automl.show_models(), indent=4)
    predictions = automl.predict(X_test)
    # Print statistics about the auto-sklearn run such as number of
    # iterations, number of models failed with a time out.
    print(automl.sprint_statistics())
    print("Accuracy score", sklearn.metrics.accuracy_score(y_test, predictions))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.13/x64/lib/python3.8/site-packages/smac/intensification/parallel_scheduling.py:153: UserWarning: SuccessiveHalving is executed with 1 workers only. Consider to use pynisher to use all available workers.
      warnings.warn(
    {   2: {   'balancing': Balancing(random_state=1),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad976160>,
               'cost': 0.021276595744680882,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa76ae80>,
               'ensemble_weight': 0.12,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad9762e0>,
               'model_id': 2,
               'rank': 1,
               'sklearn_classifier': RandomForestClassifier(max_features=5, n_estimators=64, n_jobs=1,
                           random_state=1, warm_start=True)},
        3: {   'balancing': Balancing(random_state=1),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aacb9cfd0>,
               'cost': 0.028368794326241176,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aa41c5040>,
               'ensemble_weight': 0.02,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aa61c1af0>,
               'model_id': 3,
               'rank': 2,
               'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=23, min_samples_leaf=7,
                           n_estimators=64, n_jobs=1, random_state=1,
                           warm_start=True)},
        8: {   'balancing': Balancing(random_state=1, strategy='weighting'),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaa124ee0>,
               'cost': 0.021276595744680882,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aa9bf69d0>,
               'ensemble_weight': 0.08,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa124850>,
               'model_id': 8,
               'rank': 6,
               'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=512, n_jobs=1,
                           random_state=1, warm_start=True)},
        12: {   'balancing': Balancing(random_state=1),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aacba64c0>,
                'cost': 0.028368794326241176,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad445af0>,
                'ensemble_weight': 0.1,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aacba6fa0>,
                'model_id': 12,
                'rank': 10,
                'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=1, min_samples_leaf=6,
                           min_samples_split=13, n_estimators=512, n_jobs=1,
                           random_state=1, warm_start=True)},
        19: {   'balancing': Balancing(random_state=1),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aab1cf6d0>,
                'cost': 0.028368794326241176,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad918700>,
                'ensemble_weight': 0.1,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aab1cfaf0>,
                'model_id': 19,
                'rank': 11,
                'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=13,
                           min_samples_leaf=13, n_estimators=64, n_jobs=1,
                           random_state=1, warm_start=True)},
        21: {   'balancing': Balancing(random_state=1),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad305a60>,
                'cost': 0.04255319148936165,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9ac0e11d30>,
                'ensemble_weight': 0.1,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad305490>,
                'model_id': 21,
                'rank': 12,
                'sklearn_classifier': RandomForestClassifier(bootstrap=False, max_features=4, min_samples_leaf=16,
                           min_samples_split=3, n_estimators=64, n_jobs=1,
                           random_state=1, warm_start=True)},
        22: {   'balancing': Balancing(random_state=1, strategy='weighting'),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaa824ac0>,
                'cost': 0.028368794326241176,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad337640>,
                'ensemble_weight': 0.02,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa824880>,
                'model_id': 22,
                'rank': 13,
                'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=3,
                           min_samples_leaf=4, min_samples_split=5, n_estimators=64,
                           n_jobs=1, random_state=1, warm_start=True)}}
    auto-sklearn results:
      Dataset name: breast_cancer
      Metric: accuracy
      Best validation score: 0.985816
      Number of target algorithm runs: 28
      Number of successful target algorithm runs: 16
      Number of crashed target algorithm runs: 12
      Number of target algorithms that exceeded the time limit: 0
      Number of target algorithms that exceeded the memory limit: 0

    Accuracy score 0.9440559440559441




.. GENERATED FROM PYTHON SOURCE LINES 131-133

We can also use cross-validation with successive halving
========================================================

.. GENERATED FROM PYTHON SOURCE LINES 133-168

.. code-block:: default


    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=1, shuffle=True
    )

    automl = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=40,
        per_run_time_limit=10,
        tmp_folder="/tmp/autosklearn_sh_example_tmp_01",
        disable_evaluator_output=False,
        resampling_strategy="cv",
        include={
            "classifier": [
                "extra_trees",
                "gradient_boosting",
                "random_forest",
                "sgd",
                "passive_aggressive",
            ],
            "feature_preprocessor": ["no_preprocessing"],
        },
        get_smac_object_callback=get_smac_object_callback("iterations"),
    )
    automl.fit(X_train, y_train, dataset_name="breast_cancer")

    # Print the final ensemble constructed by auto-sklearn.
    pprint(automl.show_models(), indent=4)
    automl.refit(X_train, y_train)
    predictions = automl.predict(X_test)
    # Print statistics about the auto-sklearn run such as number of
    # iterations, number of models failed with a time out.
    print(automl.sprint_statistics())
    print("Accuracy score", sklearn.metrics.accuracy_score(y_test, predictions))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.13/x64/lib/python3.8/site-packages/smac/intensification/parallel_scheduling.py:153: UserWarning: SuccessiveHalving is executed with 1 workers only. Consider to use pynisher to use all available workers.
      warnings.warn(
    {   8: {   'cost': 0.039906103286385,
               'ensemble_weight': 0.06,
               'estimators': [   {   'balancing': Balancing(random_state=1, strategy='weighting'),
                                     'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaa3426d0>,
                                     'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad406a30>,
                                     'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa342160>,
                                     'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=256, n_jobs=1,
                           random_state=1, warm_start=True)},
                                 {   'balancing': Balancing(random_state=1, strategy='weighting'),
                                     'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad976ac0>,
                                     'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aacd6c760>,
                                     'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad976520>,
                                     'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=256, n_jobs=1,
                           random_state=1, warm_start=True)},
                                 {   'balancing': Balancing(random_state=1, strategy='weighting'),
                                     'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaabe2e20>,
                                     'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa1c78b0>,
                                     'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaabe2ee0>,
                                     'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=256, n_jobs=1,
                           random_state=1, warm_start=True)},
                                 {   'balancing': Balancing(random_state=1, strategy='weighting'),
                                     'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaa7f2370>,
                                     'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aacadda60>,
                                     'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa7f24c0>,
                                     'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=256, n_jobs=1,
                           random_state=1, warm_start=True)},
                                 {   'balancing': Balancing(random_state=1, strategy='weighting'),
                                     'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aa645ce80>,
                                     'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad5928b0>,
                                     'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aa645c8e0>,
                                     'sklearn_classifier': RandomForestClassifier(bootstrap=False, criterion='entropy', max_features=4,
                           min_samples_split=4, n_estimators=256, n_jobs=1,
                           random_state=1, warm_start=True)}],
               'model_id': 8,
               'rank': 3,
               'voting_model': VotingClassifier(estimators=None, voting='soft')}}
    auto-sklearn results:
      Dataset name: breast_cancer
      Metric: accuracy
      Best validation score: 0.962441
      Number of target algorithm runs: 11
      Number of successful target algorithm runs: 5
      Number of crashed target algorithm runs: 5
      Number of target algorithms that exceeded the time limit: 1
      Number of target algorithms that exceeded the memory limit: 0

    Accuracy score 0.958041958041958




.. GENERATED FROM PYTHON SOURCE LINES 169-171

Use an iterative fit cross-validation with successive halving
=============================================================

.. GENERATED FROM PYTHON SOURCE LINES 171-206

.. code-block:: default


    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=1, shuffle=True
    )

    automl = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=40,
        per_run_time_limit=10,
        tmp_folder="/tmp/autosklearn_sh_example_tmp_cv_02",
        disable_evaluator_output=False,
        resampling_strategy="cv-iterative-fit",
        include={
            "classifier": [
                "extra_trees",
                "gradient_boosting",
                "random_forest",
                "sgd",
                "passive_aggressive",
            ],
            "feature_preprocessor": ["no_preprocessing"],
        },
        get_smac_object_callback=get_smac_object_callback("iterations"),
    )
    automl.fit(X_train, y_train, dataset_name="breast_cancer")

    # Print the final ensemble constructed by auto-sklearn.
    pprint(automl.show_models(), indent=4)
    automl.refit(X_train, y_train)
    predictions = automl.predict(X_test)
    # Print statistics about the auto-sklearn run such as number of
    # iterations, number of models failed with a time out.
    print(automl.sprint_statistics())
    print("Accuracy score", sklearn.metrics.accuracy_score(y_test, predictions))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.13/x64/lib/python3.8/site-packages/smac/intensification/parallel_scheduling.py:153: UserWarning: SuccessiveHalving is executed with 1 workers only. Consider to use pynisher to use all available workers.
      warnings.warn(
    {   2: {   'balancing': Balancing(random_state=1),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad4d9040>,
               'cost': 0.046948356807511755,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa1244f0>,
               'ensemble_weight': 0.06,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad4d9250>,
               'model_id': 2,
               'rank': 1,
               'sklearn_classifier': None},
        8: {   'balancing': Balancing(random_state=1, strategy='weighting'),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aaa813970>,
               'cost': 0.039906103286385,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad4d9280>,
               'ensemble_weight': 0.02,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa8138b0>,
               'model_id': 8,
               'rank': 4,
               'sklearn_classifier': None}}
    auto-sklearn results:
      Dataset name: breast_cancer
      Metric: accuracy
      Best validation score: 0.962441
      Number of target algorithm runs: 10
      Number of successful target algorithm runs: 5
      Number of crashed target algorithm runs: 5
      Number of target algorithms that exceeded the time limit: 0
      Number of target algorithms that exceeded the memory limit: 0

    Accuracy score 0.951048951048951




.. GENERATED FROM PYTHON SOURCE LINES 207-209

Next, we see the use of subsampling as a budget in Auto-sklearn
===============================================================

.. GENERATED FROM PYTHON SOURCE LINES 209-237

.. code-block:: default


    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=1, shuffle=True
    )

    automl = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=40,
        per_run_time_limit=10,
        tmp_folder="/tmp/autosklearn_sh_example_tmp_03",
        disable_evaluator_output=False,
        # 'holdout' with 'train_size'=0.67 is the default argument setting
        # for AutoSklearnClassifier. It is explicitly specified in this example
        # for demonstrational purpose.
        resampling_strategy="holdout",
        resampling_strategy_arguments={"train_size": 0.67},
        get_smac_object_callback=get_smac_object_callback("subsample"),
    )
    automl.fit(X_train, y_train, dataset_name="breast_cancer")

    # Print the final ensemble constructed by auto-sklearn.
    pprint(automl.show_models(), indent=4)
    predictions = automl.predict(X_test)
    # Print statistics about the auto-sklearn run such as number of
    # iterations, number of models failed with a time out.
    print(automl.sprint_statistics())
    print("Accuracy score", sklearn.metrics.accuracy_score(y_test, predictions))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.13/x64/lib/python3.8/site-packages/smac/intensification/parallel_scheduling.py:153: UserWarning: SuccessiveHalving is executed with 1 workers only. Consider to use pynisher to use all available workers.
      warnings.warn(
    {   2: {   'balancing': Balancing(random_state=1),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad271b50>,
               'cost': 0.028368794326241176,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa8243d0>,
               'ensemble_weight': 0.08000000000000002,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad271940>,
               'model_id': 2,
               'rank': 4,
               'sklearn_classifier': RandomForestClassifier(max_features=5, n_estimators=512, n_jobs=1,
                           random_state=1, warm_start=True)},
        5: {   'balancing': Balancing(random_state=1, strategy='weighting'),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aad406790>,
               'cost': 0.03546099290780147,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aa644ca00>,
               'ensemble_weight': 0.12000000000000001,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aad45e430>,
               'model_id': 5,
               'rank': 5,
               'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=3, min_samples_leaf=2,
                           n_estimators=512, n_jobs=1, random_state=1,
                           warm_start=True)},
        8: {   'balancing': Balancing(random_state=1, strategy='weighting'),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aab14b670>,
               'cost': 0.028368794326241176,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa342a90>,
               'ensemble_weight': 0.060000000000000005,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaca401c0>,
               'model_id': 8,
               'rank': 6,
               'sklearn_classifier': RandomForestClassifier(max_features=3, min_samples_leaf=2, n_estimators=512,
                           n_jobs=1, random_state=1, warm_start=True)},
        10: {   'balancing': Balancing(random_state=1, strategy='weighting'),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aa621e8e0>,
                'cost': 0.028368794326241176,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa231ca0>,
                'ensemble_weight': 0.20000000000000004,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aacf666d0>,
                'model_id': 10,
                'rank': 9,
                'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=4, min_samples_split=6,
                           n_estimators=512, n_jobs=1, random_state=1,
                           warm_start=True)},
        11: {   'balancing': Balancing(random_state=1),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aacafa6a0>,
                'cost': 0.05673758865248224,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa8131f0>,
                'ensemble_weight': 0.08000000000000002,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aacafa9d0>,
                'model_id': 11,
                'rank': 10,
                'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=23, min_samples_leaf=7,
                           n_estimators=512, n_jobs=1, random_state=1,
                           warm_start=True)}}
    auto-sklearn results:
      Dataset name: breast_cancer
      Metric: accuracy
      Best validation score: 0.978723
      Number of target algorithm runs: 22
      Number of successful target algorithm runs: 10
      Number of crashed target algorithm runs: 11
      Number of target algorithms that exceeded the time limit: 1
      Number of target algorithms that exceeded the memory limit: 0

    Accuracy score 0.9230769230769231




.. GENERATED FROM PYTHON SOURCE LINES 238-242

Mixed budget approach
=====================
Finally, there's a mixed budget type which uses iterations where possible and
subsamples otherwise

.. GENERATED FROM PYTHON SOURCE LINES 242-272

.. code-block:: default


    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)
    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
        X, y, random_state=1, shuffle=True
    )

    automl = autosklearn.classification.AutoSklearnClassifier(
        time_left_for_this_task=40,
        per_run_time_limit=10,
        tmp_folder="/tmp/autosklearn_sh_example_tmp_04",
        disable_evaluator_output=False,
        # 'holdout' with 'train_size'=0.67 is the default argument setting
        # for AutoSklearnClassifier. It is explicitly specified in this example
        # for demonstrational purpose.
        resampling_strategy="holdout",
        resampling_strategy_arguments={"train_size": 0.67},
        include={
            "classifier": ["extra_trees", "gradient_boosting", "random_forest", "sgd"]
        },
        get_smac_object_callback=get_smac_object_callback("mixed"),
    )
    automl.fit(X_train, y_train, dataset_name="breast_cancer")

    # Print the final ensemble constructed by auto-sklearn.
    pprint(automl.show_models(), indent=4)
    predictions = automl.predict(X_test)
    # Print statistics about the auto-sklearn run such as number of
    # iterations, number of models failed with a time out.
    print(automl.sprint_statistics())
    print("Accuracy score", sklearn.metrics.accuracy_score(y_test, predictions))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.8.13/x64/lib/python3.8/site-packages/smac/intensification/parallel_scheduling.py:153: UserWarning: SuccessiveHalving is executed with 1 workers only. Consider to use pynisher to use all available workers.
      warnings.warn(
    {   2: {   'balancing': Balancing(random_state=1),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aa9c34220>,
               'cost': 0.028368794326241176,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa52d070>,
               'ensemble_weight': 0.22,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aaa993400>,
               'model_id': 2,
               'rank': 4,
               'sklearn_classifier': RandomForestClassifier(max_features=5, n_estimators=512, n_jobs=1,
                           random_state=1, warm_start=True)},
        7: {   'balancing': Balancing(random_state=1, strategy='weighting'),
               'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9aac9e4c10>,
               'cost': 0.03546099290780147,
               'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aaa124790>,
               'ensemble_weight': 0.06,
               'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aac9e40d0>,
               'model_id': 7,
               'rank': 5,
               'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=4, min_samples_split=6,
                           n_estimators=64, n_jobs=1, random_state=1,
                           warm_start=True)},
        17: {   'balancing': Balancing(random_state=1),
                'classifier': <autosklearn.pipeline.components.classification.ClassifierChoice object at 0x7f9ac1082df0>,
                'cost': 0.07801418439716312,
                'data_preprocessor': <autosklearn.pipeline.components.data_preprocessing.DataPreprocessorChoice object at 0x7f9aad1f1640>,
                'ensemble_weight': 0.04,
                'feature_preprocessor': <autosklearn.pipeline.components.feature_preprocessing.FeaturePreprocessorChoice object at 0x7f9aacd6ccd0>,
                'model_id': 17,
                'rank': 6,
                'sklearn_classifier': RandomForestClassifier(criterion='entropy', max_features=16, n_estimators=64,
                           n_jobs=1, random_state=1, warm_start=True)}}
    auto-sklearn results:
      Dataset name: breast_cancer
      Metric: accuracy
      Best validation score: 0.978723
      Number of target algorithm runs: 28
      Number of successful target algorithm runs: 13
      Number of crashed target algorithm runs: 14
      Number of target algorithms that exceeded the time limit: 1
      Number of target algorithms that exceeded the memory limit: 0

    Accuracy score 0.951048951048951





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  7.060 seconds)


.. _sphx_glr_download_examples_60_search_example_successive_halving.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/automl/auto-sklearn/master?urlpath=lab/tree/notebooks/examples/60_search/example_successive_halving.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: example_successive_halving.py <example_successive_halving.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: example_successive_halving.ipynb <example_successive_halving.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
